# 2_analista_ia.py (Vers√£o Final com Valida√ß√£o)AIzaSyAjA_YoXVBXJ3e5QVeyA1zPv5ivzWfnrg4
# 2_analista_ia.py (Vers√£o Final com Valida√ß√£o e Processamento em Lotes)
import requests
from bs4 import BeautifulSoup
import json
import os
import google.generativeai as genai
import time
from urllib.parse import urlparse
import os

# --- CONFIGURA√á√ÉO ---
# Cole sua chave de API do Google AI aqui.
# Voc√™ pode obter uma em https://aistudio.google.com/
CHAVE_API_GOOGLE = os.getenv("GOOGLE_API_KEY")
ARQUIVO_DE_SITES = 'sites_para_analisar.txt'
ARQUIVO_DE_SAIDA = 'configuracao_scrapers.json'
ARQUIVO_ERROS = 'relatorio_erros_analista.txt'
# --------------------

if CHAVE_API_GOOGLE == "SUA_CHAVE_API_AQUI":
    print("‚ö†Ô∏è ATEN√á√ÉO: Voc√™ precisa adicionar sua chave de API do Google AI na vari√°vel CHAVE_API_GOOGLE.")
    exit()

genai.configure(api_key=CHAVE_API_GOOGLE)

def carregar_configuracoes_existentes(arquivo):
    """Carrega as configura√ß√µes j√° salvas, se o arquivo existir."""
    if os.path.exists(arquivo):
        try:
            with open(arquivo, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            print(f"Aviso: O arquivo '{arquivo}' est√° corrompido ou vazio. Come√ßando um novo.")
            return {}
    return {}

def buscar_html_da_pagina(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        return response.text, None
    except requests.RequestException as e:
        return None, f"Erro ao buscar a URL: {e}"

def criar_prompt_para_llm(html_pagina):
    soup = BeautifulSoup(html_pagina, 'html.parser')
    for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'svg']):
        tag.decompose()
    html_limpo = soup.prettify()[:15000] # Limita o tamanho para a API

    return f"""
    Analise o c√≥digo HTML a seguir de uma p√°gina de listagem de im√≥veis e gere um JSON com os seletores CSS para os seguintes campos:
    1. 'container_anuncio': O seletor principal que engloba um √∫nico an√∫ncio de im√≥vel na lista.
    2. 'preco': Seletor para o pre√ßo do im√≥vel.
    3. 'endereco': Seletor para o endere√ßo ou bairro.
    4. 'descricao': Seletor para o bloco de texto com a descri√ß√£o do im√≥vel (onde podem estar quartos, banheiros, etc.).
    5. 'proxima_pagina': Seletor para o link ou bot√£o de "pr√≥xima p√°gina" ou "next". Se n√£o encontrar, retorne null.

    Instru√ß√µes:
    - Forne√ßa os seletores DENTRO do 'container_anuncio', exceto para 'proxima_pagina'.
    - O formato da sa√≠da deve ser um JSON v√°lido, e nada mais.

    HTML:
    ```html
    {html_limpo}
    ```
    JSON de sa√≠da:
    """

def gerar_configuracao_com_ia(html_pagina):
    prompt = criar_prompt_para_llm(html_pagina)
    try:
        print("  üß† Enviando HTML para o Google AI (Gemini) analisar...")
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        response = model.generate_content(prompt)
        # Extrai e limpa a resposta para garantir que seja apenas o JSON
        resposta_json = response.text.strip().replace('```json', '').replace('```', '')
        return json.loads(resposta_json), None
    except Exception as e:
        if 'quota' in str(e).lower():
            return None, "Cota da API excedida"
        else:
            return None, f"Erro ao comunicar com a API: {e}"

# --- FLUXO PRINCIPAL ---
if __name__ == "__main__":
    if not os.path.exists(ARQUIVO_DE_SITES):
        print(f"Arquivo '{ARQUIVO_DE_SITES}' n√£o encontrado. Execute o '1_buscar_sites.py' primeiro.")
        exit()

    configuracoes_finais = carregar_configuracoes_existentes(ARQUIVO_DE_SAIDA)
    erros_analise = []
    print(f"Carregadas {len(configuracoes_finais)} configura√ß√µes j√° existentes.")

    with open(ARQUIVO_DE_SITES, 'r', encoding='utf-8') as f:
        sites_para_processar = [line.strip() for line in f if line.strip()]

    sites_novos = [site for site in sites_para_processar if urlparse(site).netloc not in configuracoes_finais]
    
    if not sites_novos:
        print("‚úÖ Nenhum site novo para processar.")
        exit()

    print(f"ü§ñ Iniciando an√°lise com IA para {len(sites_novos)} novos sites...")

    for site_url in sites_novos:
        dominio = urlparse(site_url).netloc
        print(f"\nProcessando: {site_url}")
        html_result, erro_busca = buscar_html_da_pagina(site_url)
        
        if erro_busca:
            erros_analise.append(f"{site_url} - {erro_busca}")
            continue

        time.sleep(2)
        config, erro_ia = gerar_configuracao_com_ia(html_result)
        
        if erro_ia:
            erros_analise.append(f"{site_url} - {erro_ia}")
            if "Cota da API excedida" in erro_ia:
                print("  üõë Parando a execu√ß√£o. Rode novamente mais tarde para continuar.")
                break
            continue

        if isinstance(config, dict) and config.get('container_anuncio'):
            print(f"  ‚úÖ Configura√ß√£o gerada e validada!")
            config['url_busca'] = site_url
            configuracoes_finais[dominio] = config
        else:
            motivo = f"A IA n√£o retornou uma configura√ß√£o v√°lida. Resposta: {config}"
            print(f"  ‚ö†Ô∏è AVISO: {motivo}")
            erros_analise.append(f"{site_url} - {motivo}")

    with open(ARQUIVO_DE_SAIDA, 'w', encoding='utf-8') as f:
        json.dump(configuracoes_finais, f, indent=2, ensure_ascii=False)
    print(f"\n\nüéâ Processo finalizado! {len(configuracoes_finais)} configs salvas em '{ARQUIVO_DE_SAIDA}'")

    if erros_analise:
        with open(ARQUIVO_ERROS, 'w', encoding='utf-8') as f:
            f.write("Relat√≥rio de Erros - Rob√¥ Analista com IA\n" + "="*40 + "\n")
            for erro in erros_analise:
                f.write(f"{erro}\n")
        print(f"\nüìù Um relat√≥rio com {len(erros_analise)} erros foi salvo em '{ARQUIVO_ERROS}'")

